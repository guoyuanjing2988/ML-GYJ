{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In the previous lesson we covered how to extract features from raw input so that the machine can understand. In this lesson, we will introduce some basic algorithms for machine learning. The common goal for these algorithms is using features and labels to generate a model that can predict a good result for coming events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Logistic Regression"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Logistic regression is always considered as the easiest machine learning model. However it often gives a good result at a short time cost. It is also the foundation for other models. Therefore logistic regression is always a good place to start."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Logistic Regression is implemented in scikit learn toolkit. The offical website is http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "There are many parameters for LogisticRegression(). The most important one is the constant C(default=1.0). C is inverse of regularization strength. It suggests whether the model is tight or loose. The concept is hard to understand, therefore just see the example below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tr>\n",
    "    <td> <img src=\"img/raw.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "    <td> <img src=\"img/good.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tr>\n",
    "    <td> <img src=\"img/bad.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "    <td> <img src=\"img/bad2.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In the above example, the first picture is the raw data. There are two classes and we want to build a classifier to separate them. The second picture is good example. Though there are some errors, the whole picture is good enough. This is the case when C=1.0. When we tune C=1000.0, the model will become the third picture. We can see that the model is overfitting badly. It wont work well for new tests. The fourth picture is the case for C=0.01. The model is bad. In conclusion, when C is too large, it will lead to overfitting; when C is too small, it will lead to loose model. Therefore choosing a proper C is very important(it may not always be 1.0). "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "After define a logistic regression model, like:\n",
    "model=LogisticRegression(C=0.1)\n",
    "We need to train the model, by use the fit function:\n",
    "model.fit(input,output)\n",
    "When it is done, we can use the predict function to predict for new inputs:\n",
    "result=model.predict(new_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. K-Nearest Neighbour (KNN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
