{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In the previous lesson we covered how to extract features from raw input so that the machine can understand. In this lesson, we will introduce some basic algorithms for machine learning. The common goal for these algorithms is using features and labels to generate a model that can predict a good result for coming events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Logistic Regression"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Logistic regression is always considered as the easiest machine learning model. However it often gives a good result at a short time cost. It is also the foundation for other models. Therefore logistic regression is always a good place to start."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Logistic Regression is implemented in scikit learn toolkit. The offical website is http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "There are many parameters for LogisticRegression(). The most important one is the constant C(default=1.0). C is inverse of regularization strength. It suggests whether the model is tight or loose. The concept is hard to understand, therefore just see the example below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tr>\n",
    "    <td> <img src=\"img/raw.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "    <td> <img src=\"img/good.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tr>\n",
    "    <td> <img src=\"img/bad.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "    <td> <img src=\"img/bad2.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In the above example, the first picture is the raw data. There are two classes and we want to build a classifier to separate them. The second picture is good example. Though there are some errors, the whole picture is good enough. This is the case when C=1.0. When we tune C=1000.0, the model will become the third picture. We can see that the model is overfitting badly. It wont work well for new tests. The fourth picture is the case for C=0.01. The model is bad. In conclusion, when C is too large, it will lead to overfitting; when C is too small, it will lead to loose model. Therefore choosing a proper C is very important(it may not always be 1.0). "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "After define a logistic regression model, like:\n",
    "model=LogisticRegression(C=0.1)\n",
    "We need to train the model, by use the fit function:\n",
    "model.fit(input,output)\n",
    "When it is done, we can use the predict function to predict for new inputs:\n",
    "result=model.predict(new_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. K-Nearest Neighbour (KNN) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "K-Nearest Neighbour is regarded as the easiest model to understand in machine learning. To predict the value for a test data, we simply compare it to k nearest points in the vector space and take the majority class. It is more clear to see with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<td> <img src=\"img/knn.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The coloured dots are training datas, they are labeled in three classes. To predict which class does a random point in the graph belongs, we simply find the first k nearest points. For example, if at a point where we take first 5 nearest points, and three of them are green, one is red and one is blue, we will mark this point as green."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The KNN method is implemented in scikit learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[[ 0.66666667  0.33333333]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "X = [[0], [1], [2], [3]]\n",
    "y = [0, 0, 1, 1]\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(X, y) \n",
    "print(neigh.predict([[1.1]]))\n",
    "print(neigh.predict_proba([[0.9]]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In the example above, we have four training points. For a new testing point, we take the first 3 nearest points. For x=1.1, the nearest points are 0,1,2, and 0,1 belongs to class 0; 2 belongs to class 1. Therefore the result for x=1.1 is 0. The function predict_proba returns the percentage of a class in the nearest neighbours. There are 2 points in class 0 and 1 point in class 1. Therefore the percentage is [2/3,1/3]=[0.667,0.333]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Although K-Nearest Neighbour is easy to use, it doesn't perform so well in most cases, because it requires a good preprocessing of datas. For example if we want to classify digits, we have to make sure that the digit in the picture is in the center and upright for KNN to work well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Support Vector Machine (SVM) is a supervised machine learning model which can be used for classification and regression. SVM constructs one or multiple hyperplanes in a higher dimension and build the classifier. The advantage is that usually it is very hard to separate perfectly for a training data, and add a few more dimensions can help a lot. For the separation of different classes, SVM looks for the maximum-margin hyperplane. That is easy to understand with the following picture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<td> <img src=\"img/svmmargin.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In the above picture, H1 is not a good separation, H2 separates the two classes, but the distance between the line and the points are close, while H3 is the perfect line because it has the maximum margin. This also applies to higher dimension."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SVM is also implemented in scikit learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "X = [[0, 0], [1, 1]]\n",
    "y = [0, 1]\n",
    "clf = svm.SVC()\n",
    "clf.fit(X, y) \n",
    "print(clf.predict([[2,2]]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In the example above,there are two dots representing two classes.Because (2,2) is on the (1,1) side, it is classified as class 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SVM is a powerful model compared to the models mentioned above. The disadvantge is that SVM is much slower compared to logistic regression and KNN, and therefore not applicable in some cases."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Logistic Regression, K-Nearest Neighbour and Support Vector Machine are most commonly used machine learning models. They are easy to use at low time complexity. However their accuracy may not be as high as more complex models, which will be introduced in next class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
