{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As previously mentioned, there are two main types of machine learning: supervised and unsupervised. Supervised learning requires both input and output, while unsupervised learning only needs the input data.\n",
    "\n",
    "However, the input data we usually have can't be understood by computer directly. For example, machine can't store pictures as a whole block, but it stores them as numbers representing pixels. Therefore when we get the input data as pictures, texts and audio, we need to do Feature Extraction so that we can give computer the input it understands.\n",
    "\n",
    "Let's use text file as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_text='I love Machine Learning.'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We have to transform the sentence somehow into numbers so that the computer can perform operations on it.\n",
    "The easiest way is to use the ASCII code, which is a correlation between numbers and charaters inside computer.\n",
    "For every character, there is an integer between 0-255 representing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[73, 32, 108, 111, 118, 101, 32, 77, 97, 99, 104, 105, 110, 101, 32, 76, 101, 97, 114, 110, 105, 110, 103, 46]\n"
     ]
    }
   ],
   "source": [
    "sample_ascii_code_from_text=[ord(character) for character in sample_text]\n",
    "print(sample_ascii_code_from_text)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now we get a number sequence that computer can understand! This number sequence is called a feature, and the method is called feature extraction. It is one of the most important processes in machine learning. In this lesson we will mainly focus on texts and pictures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Feature Extraction"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The smallest building block for a text is characters. However characters alone usually don't have meanings. Therefore in text feature extraction, it is more common to use words as building blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Count Vectorizer "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The most obvious way to represent sentences by words is a list with number of times each word appears. This method is called count vectorizing. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_text='I love machine learning, and I love sports.'\n",
    "sample_vocabulary=['I','love','machine','learning','and','sports']\n",
    "sample_count_vector=[2,2,1,1,1,1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In the example, because 'I' appears two times in the text, therefore the number corresponding is 2, while the number for 'machine' is 1.\n",
    "Count Vectorizer is implemented in scikit learn python library. It can be used by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'approach', 'between', 'but', 'can', 'commits', 'consuming', 'course', 'didn', 'do', 'fix', 'fixing', 'fresh', 'git', 'have', 'if', 'in', 'is', 'it', 'little', 'make', 'my', 'of', 'only', 'parallel', 'problem', 'problems', 'pull', 'race', 'repository', 'svn', 'the', 'this', 'time', 'to', 'trying', 'unpushed', 'usual', 'with', 'works', 'you']\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 3, 1, 1, 1, 1, 1, 3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "sample_text=\"My usual approach to fixing git-svn problems is to make a fresh pull of the repository. Time consuming, but you can do it in parallel with trying to fix the problem. Have a little race between you and git. Of course, this only works if you didn't have unpushed commits.\"\n",
    "count_vectorizer=CountVectorizer()\n",
    "sample_count_vector=count_vectorizer.fit_transform([sample_text]).todense().tolist()[0]\n",
    "print(sorted(count_vectorizer.vocabulary_,key=count_vectorizer.vocabulary_.get))\n",
    "print(sample_count_vector)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In the previous example we can see that the count vectorizer returns a count vector correspond to words appears in the text.\n",
    "In the count vectorizer scikit learn provided, there are many useful parameters we can use. For example we can specify the vocabulary instead of the whole vocabulary of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['approach', 'you', 'like']\n",
      "[1, 3, 0]\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer_with_vocab=CountVectorizer(vocabulary=['approach','you','like'])\n",
    "sample_count_vector=count_vectorizer_with_vocab.fit_transform([sample_text]).todense().tolist()[0]\n",
    "print(sorted(count_vectorizer_with_vocab.vocabulary_,key=count_vectorizer_with_vocab.vocabulary_.get))\n",
    "print(sample_count_vector)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "There are a lot other parameters. Further information can be found in \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.TfIdf Vectorizer "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Count Vectorizer is easy to understand, but it doesn't work so well in most cases. One reason is because every word is labeled only with the count. However in many cases when we try to extract import information from text, we want to get rid of unrelevant but common words, such as 'but', 'because'. For example, for the following sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences=['There is a pear, a peach and a watermelon',\n",
    "          'I want a banana']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The count vector for the second sentence is [1,1,1,1]. However we can clearly see that the word 'a' isn't our focus. The word 'banana' is more important. Therefore we want to make the weightage of word higher, instead of the same. One of the approaches is using TfIdf Vectorizer, which is also implemented in scikit learn. This method give the words that appear fewer times a higher weightage, and lower weightage for more frequent words. Below is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.5694308628404254, 0.0, 0.43306684852870914, 0.43306684852870914, 0.43306684852870914, 0.33631504064053513]\n",
      "['and', 'design', 'national', 'of', 'singapore', 'technology', 'university']\n"
     ]
    }
   ],
   "source": [
    "sentences=['National University of Singapore',\n",
    "          'National Technology University',\n",
    "          'Singapore University of Technology and Design']\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=1)\n",
    "tfidf_vectorizer.fit(sentences)\n",
    "print(tfidf_vectorizer.transform(['Singapore University of Technology Design']).todense().tolist()[0])\n",
    "print(sorted(tfidf_vectorizer.vocabulary_,key=tfidf_vectorizer.vocabulary_.get))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "From the result we can see that although the vocabulary is the same, the weightage are different. Because 'university' appears three times in all sentences, the weightage is the lowest. Words like 'design' only appears once, and it has the highest weightage. TfIdf Vectorizer is very useful in information extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
